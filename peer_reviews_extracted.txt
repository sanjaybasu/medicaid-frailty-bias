Now I'll produce a rigorous peer review from each of the four expert perspectives. Let me craft detailed, genuinely critical reviews that don't simply validate the work.

---

# DETAILED PEER REVIEW: "Racial and Geographic Disparities in Medically Frail Exemption Rates Under Medicaid Work Requirements"

---

## REVIEWER 1: Alan Weil, Health Affairs Editor-in-Chief

### MAJOR CONCERNS

**1. Policy relevance claim overstates current applicability — OBBBA is speculative**

The manuscript frames itself as analyzing the "One Big Beautiful Bill Act (OBBBA)" mandates, but this statute passed in February 2025 and explicitly defers implementation details to state discretion. The work requirement programs analyzed (Georgia, Indiana, Arkansas, Montana, North Carolina) predate OBBBA and were implemented under different statutory authorities (Section 1115 waivers under ACA § 1315). The connection between the historical waiver programs and the future OBBBA implementation is asserted but not rigorously established. Will OBBBA require the same claims-based frailty operationalization as these antecedent programs? The manuscript does not establish this. This significantly weakens the policy relevance for a 2026 audience making prospective policy decisions about OBBBA.

**Specific editorial recommendation:** Reframe the abstract and Introduction to state explicitly: "This analysis examines historical Medicaid work requirement programs that predate OBBBA and may inform prospective implementation," rather than presenting it as a direct OBBBA analysis. The policy implications section should be conditional on the assumption that OBBBA implementation follows the administrative patterns of these earlier programs—an assumption that requires justification or explicit limitation.

**2. The "so what" for policy is insufficiently developed — who decides what happens with these findings?**

The manuscript documents a 4.81 pp racial gap in exemption rates and projects 223,833 excess coverage losses. For a Health Affairs policy audience, the critical missing piece is: **who has statutory authority and enforcement levers to fix this?** The policy implications section prescribes three changes (HIE integration mandate, ex parte determination requirement, algorithmic impact assessments) but does not address:
- Does CMS have explicit statutory authority under OBBBA (or under current ACA § 1115 waiver guidance) to mandate these conditions?
- What is the political feasibility? States have historically resisted CMS micromanagement of operational definitions.
- Are there counterarguments from states that might prefer claims-based systems (cost, simplicity)?
- If a state refuses to implement these changes, what is the enforcement mechanism?

**Specific editorial recommendation:** Add a subsection to "Policy Implications" titled "Implementation Pathways and Political Feasibility" that directly addresses the governance question: "CMS could impose these requirements through: (1) updated waiver approval guidance (authority: 42 CFR § 430.25); (2) Section 1115 Special Terms and Conditions; or (3) direct OBBBA rulemaking if Congress explicitly delegates frailty definition authority to HHS." Acknowledge state-level pushback likely to materialize.

**3. The abstract's claim that state systems "may systematically underidentify frailty" is conceptually muddled**

The abstract states: "states are implementing claims-based administrative systems—many relying on billing data as a proxy for health need—that may systematically underidentify frailty in populations with suppressed healthcare utilization." But the paper conflates two distinct questions:

- **Question A:** Are claims-based systems poor *proxy measures* of health need? (Yes, Obermeyer et al. answered this)
- **Question B:** Are the specific state frailty definitions, as written, being correctly *operationalized* by these claims-based systems?

The manuscript demonstrates A but does not clearly establish B. It's possible that Georgia's written frailty definition is appropriate for identification of the intended population, but the state's *implementation* via ICD-10 lookups misses cases that clinical review would catch. Alternatively, Georgia's definition might be too narrow to begin with (a policy choice problem, not an implementation fidelity problem). The paper does not untangle these.

**Specific editorial recommendation:** In the Introduction, distinguish explicitly between: (i) the design of state frailty definitions (policy scope), (ii) the fidelity of implementation given the definition (administrative fidelity), and (iii) the choice of claims as the evidentiary base (systemic architecture). State which the paper addresses. Currently it blurs all three.

**4. The geographic finding (provider density → smaller gaps) is underdeveloped and possibly confounded**

The paper reports: "States with fewer personal care providers per enrollee have larger racial exemption gaps (r=−0.516, p=0.041)." This is presented as evidence of a "provider-density mediation pathway." But the causal story is speculative:

- Does lower provider density *mechanically* reduce T1019 claims (thus reducing signal for frailty identification)? Plausible.
- Or does higher provider density correlate with higher-capacity state Medicaid programs, better-resourced eligibility systems, and broader frailty definitions *independently* of T1019 density? Also plausible.
- The paper does not disentangle these. The correlation r=−0.516 is observed; the *mechanism* is inferred.

Moreover, provider density is measured at the state level. Georgia has 116 personal care providers; California has 3,200. But enrollee *access* to these providers depends on geographic distribution within states. A state with 116 providers concentrated in Atlanta differs dramatically from a state with 116 providers scattered across rural Montana. The paper acknowledges this (Limitation 7) but does not address the implication: the geographic finding may be largely driven by a handful of states with sparse rural distribution rather than a generalizable provider-density mechanism.

**Specific editorial recommendation:** Qualify the provider density finding as: "suggestive of a correlation between provider density and racial gap, but causal mechanism unconfirmed." Consider adding an exploratory analysis restricting to states with observed (not modeled) exemption rate data to assess whether the correlation holds in a smaller, higher-quality sample.

**5. Conclusions overstate the "mechanism" — this is a descriptive finding, not a mechanistic demonstration**

The Conclusions section states: "Claims-based frailty determination under OBBBA work requirements produces racial and geographic disparities in exemption rates, **reflecting a common mechanism**: healthcare utilization data is a poor proxy for health need in populations with structural barriers to care."

This is logically problematic. The paper documents: (1) Black enrollees have *lower* exemption rates despite *higher* disability burden, and (2) states using claims-based systems exist. The jump from (1) and (2) to "reflects a mechanism of suppressed utilization" is inferential, not demonstrated. Alternative explanations:

- States with less-inclusive frailty definitions (e.g., Georgia with stringency 4.2) exempt fewer people *of all races*, and racial gaps are partially a function of definition narrowness, not implementation fidelity.
- The BRFSS disability measure (self-reported any-disability) may not align with the clinical constructs in state policies, biasing the calibration test.
- Physician certification requirements might have differential documentation outcomes by race, independent of claims data.

The mechanism is plausible, but the paper does not *prove* suppressed utilization is the dominant mechanism. 

**Specific editorial recommendation:** Reframe as: "These patterns are consistent with a hypothesis that claims-based determination underidentifies individuals with suppressed utilization. Alternative explanations—including narrower state policy definitions or differential compliance with documentation requirements—cannot be ruled out with the available data."

### MINOR CONCERNS

**1. Abstract is too long and dense for the target audience**

The Health Affairs abstract (currently ~240 words) should be ~200 words maximum for journal standards and non-technical policymakers. The sentence "States with Health Information Exchange integration, ex parte determination, and higher personal care provider density show smaller disparities" is buried at the end. The actionable takeaway should appear earlier.

**Suggested edit:** Shorten to ~200 words; move the concrete policy implications (HIE, ex parte) to the abstract's final sentence.

**2. "OBBBA" is an awkward acronym and may not be widely recognized by February 2026**

Health Affairs readers may not know what OBBBA is or when it passed. 

**Suggested edit:** First mention: "the One Big Beautiful Bill Act (OBBBA, H.R. 1, 119th Congress, enacted February 2025)" in the Abstract, not just Introduction.

**3. Missing citation to recent CMS guidance on frailty determination**

The manuscript cites Section 1115 waiver documents and Sommers et al. (2019) but does not cite CMS's *official guidance* on frailty exemption operationalization under work requirements. If CMS issued guidance in 2024–2025 (prior to OBBBA), this should be cited to ground the analysis in actual CMS policy expectations.

**4. Table 1 is difficult to parse — too many columns and superscripts**

The table mixes observed and modeled exemption rates with only small superscript markers (^O, ^M). For policymakers, this distinction is crucial but easily missed.

**Suggested edit:** Separate Table 1 into two subtables: "States with Observed Exemption Rate Data (n=4)" and "States with Modeled Exemption Rates (n=12)."

**5. No discussion of equity trade-offs in work requirement programs**

The manuscript focuses narrowly on frailty determination but does not situate this within the broader debate about work requirements' effects on vulnerable populations. Some Health Affairs readers (including some with policy influence) question whether targeted frailty exemptions *ever* provide sufficient protection in the context of work requirements with high compliance burden. This manuscript might acknowledge that perspective.

### OVERALL RECOMMENDATION

**MAJOR REVISION required**

The manuscript makes important descriptive findings about racial gaps in work requirement exemption rates and situates them within an analytically sensible framework (suppressed utilization bias). However, for publication in Health Affairs, the policy relevance and actionability need substantial strengthening:

1. Clarify the temporal relationship between this historical analysis and prospective OBBBA implementation.
2. Develop the "so what" for policy by addressing CMS authority, feasibility, and enforcement.
3. Disaggregate design, fidelity, and architecture in the frailty determination problem.
4. Qualify the causal mechanism claims and address alternative explanations.

The manuscript is scientifically sound but needs editing to meet Health Affairs standards for clarity and policy-relevance for a non-technical audience.

---

## REVIEWER 2: Medicaid Policy Expert (Georgetown CCF / MACPAC)

### MAJOR CONCERNS

**1. Exemption rate data hierarchy is inadequately disclosed and the modeled rates are of dubious validity**

The paper uses *four observed* exemption rates (Georgia, Arkansas, Indiana, North Carolina) and *twelve modeled* rates extrapolated via OLS regression (R² = 0.645). This is a fundamental data quality problem that pervades the entire analysis:

- For the four observed states: Georgia's data are from the DHS Annual Evaluation Report 2024 (does this count *approved* exemptions or *applied-for* exemptions? are there pending appeals?). Arkansas's data are from Sommers et al.'s telephone survey (±2.5 pp margin of error for race-stratified estimates). Indiana's come from HIP 2.0 evaluation reports (which fiscal year? what denominator was used?). North Carolina's dates are listed as "2023–2024"—are these point estimates or averages?

- For the twelve modeled states: The paper predicts exemption rates using the OLS regression framework with n=16 observations and 6 predictors. This model has an adjusted R² of 0.225, meaning ~78% of variance is unexplained. The manuscript acknowledges this is "severely underpowered," yet all subsequent analyses (DiD, coverage loss projection) treat these modeled rates as if they were measured.

The policy expert's critical question: **How confident can we be in a finding about Louisiana's exemption gap when Louisiana's exemption rates are entirely model-predicted?** The paper footnotes this distinction but does not apply differential statistical weights or sensitivity analyses that exclude modeled-rate states.

**Specific policy recommendation:** Present all primary results twice: (1) using only observed data (n=4 states), and (2) using observed + modeled (n=16 states). If the conclusions change substantially with observed-only data, this suggests the modeled rates are driving findings rather than observed disparities. The current approach conflates measurement with prediction.

**2. State work requirement program statuses are not current as of February 2026**

The manuscript lists program statuses as of 2024 (e.g., "Montana Active, Georgia Active, Arkansas Terminated"). But the submission is dated February 2026. Has Arkansas's program been reinstated? Was North Carolina's program stayed by a subsequent court decision? Have any additional states implemented work requirements under OBBBA in the intervening year? Has the Trump administration (in office as of January 2025) changed CMS guidance on frailty determination?

The paper provides no discussion of post-2024 developments, which is a significant gap for a policy audience reading in February 2026.

**Specific policy recommendation:** Add a note: "Program statuses as reported through [date]. Subsequent changes [specify any known to author]." If there have been major program changes post-analysis period, discuss their potential implications for findings.

**3. The OBBBA frailty exemption statute is not accurately characterized**

The manuscript states (Abstract): "The statute designates 'medically frail' individuals as exempt from this requirement, defining the category to include those with serious mental illness, substance use disorder, physical or developmental disability substantially limiting activities of daily living (ADLs), or other serious or complex medical conditions."

However, OBBBA (H.R. 1, 119th Congress) does not contain a statutory definition of "medically frail." The statute, in Section 1115(f), delegates frailty definition to CMS within broad categories. The specific frailty criteria will be determined through CMS rulemaking and individual state waiver approvals, not by the statute itself. The paper's statement that "The operationalization of this exemption is left to state discretion" is correct, but the assertion that the *statute* "designates" specific criteria (SMI, SUD, etc.) is inaccurate.

**Specific policy recommendation:** Correct to: "CMS guidance and individual state Section 1115 waivers have defined 'medically frail' to include..." and cite the actual waiver documents, not the statute.

**4. The claims that states use "claims-based frailty indices" (CFIs) is overstated**

The manuscript asserts that "An emerging subset of states are deploying claims-based frailty indices (CFIs) that aggregate billing record patterns into scalar risk scores" and identifies Michigan as using such an algorithm. But the evidence presented is thin:

- Michigan is listed as using a "CFI" in Table 2, but the Methods section (A.3) provides no source document.
- No other state is definitively documented to use a full claims-based frailty index. Most states use a combination of: (i) mandatory ICD-10 code triggers, (ii) physician certification, and (iii) ex parte review of specific billing patterns (e.g., T1019 claims).

The distinction matters because a true CFI (e.g., the Gagne Comorbidity Index adapted to claims data) is a validated risk model, whereas a list of relevant ICD-10 codes + T1019 patterns is not an "algorithm" in the machine learning sense. Presenting heterogeneous administrative processes as "algorithmic" may overstate the algorithmic content and obscure the actual policy variation.

**Specific policy recommendation:** Distinguish between: (1) states using specific condition triggers (ICD-10 codes + optional T1019), (2) states aggregating conditions into weighting schemes (quasi-algorithmic), and (3) states using validated claims-based risk models (true CFI). Table 2 should clarify which category each state occupies, with source documents cited. If most states are in category (1), the title "algorithmic bias" may be misleading.

**5. The policy regression (Table 3) is presented despite acknowledged severe underpowering — this damages credibility**

Table 3 presents an OLS regression with n=16 observations and 6 predictors, which violates basic regression standards. The adjusted R² is 0.225. The manuscript acknowledges "this model is severely underpowered and individual coefficients are not reliably interpretable for causal inference."

Yet the paper then states: "Policy stringency score is inversely associated with the racial gap (β=−0.51 pp per stringency point; 95% CI: −1.14 to 0.12; p=0.099)." This CI crosses zero and p=0.099 is not significant, yet the result is presented as a finding. For a policy expert, the appearance of presenting underpowered regression results as policy guidance undermines the paper's authority.

**Specific policy recommendation:** Move Table 3 to the Appendix or remove it entirely. If retained, present only as "exploratory descriptive summary" without statistical inference language. Instead, use the observed-data subset (GA, AR, IN, NC) to describe qualitatively which policy features correlate with smaller gaps, acknowledging the n=4 limitation.

### MINOR CONCERNS

**1. Missing documentation of how policy stringency score was validated**

The Methods section (A.3) presents the policy stringency score with "prespecified weights" (ADL threshold 0.25, physician certification 0.20, etc.). But there is no validation evidence: Did any subject-matter experts review the weighting? Did the weights align with actual state policy variation? Is this scoring scheme reproducible by others?

**Suggested edit:** Cite a validation study if one exists, or add Appendix Methods C.4: "Policy Stringency Score Validation" discussing inter-rater reliability or comparison to expert judgments.

**2. Inconsistent language about "active" vs. "implemented" vs. "pending" programs**

Table 2 uses categories "Active, Pending, Blocked, Terminated, None" but the text sometimes uses these inconsistently. For instance, the Introduction states "Four states are the only states with active work requirement programs at time of analysis" (MT, MT, GA, NC), but Table 1 lists both "active" and "pending" states. For a policy audience, the operational status of each program matters.

**Suggested edit:** Define operationally: Active = currently processing exemption determinations; Pending = approved but not yet live; Blocked = injunction or denial; Terminated = repealed or suspended. Use consistently.

**3. The T1019 billing data quality issues are underplayed**

Appendix Methods A.1 mentions "cell suppression applies to provider-month cells with <12 claims, disproportionately affecting small states and rural areas" and "six states identified by KFF as having data quality concerns have suppressed or sparse 2024 data." But the impact on the analysis is not quantified. Which six states? How sparse is sparse? Did these states' exemption rates or provider density estimates change if the suppressed data were recovered?

**Suggested edit:** Appendix Methods A.1 should list by name the six states with data quality concerns and note whether any are in the primary analysis cohort (16 states with race-stratified data).

**4. No discussion of the clinical validity of BRFSS "any-disability" as a measure of "medically frail"**

The paper uses CDC BRFSS any-disability prevalence (self-reported functional limitation in six domains) as the benchmark for assessing whether states' frailty exemption rates are calibrated to true need. But BRFSS any-disability is broader than most state statutory definitions of medically frail (which typically require ADL limitation substantially limiting *multiple* activities, not any single domain).

This measurement mismatch is acknowledged as Limitation 3 but not deeply explored. If states' frailty definitions are narrower than BRFSS any-disability, the calibration test may be invalid.

**Suggested edit:** Add sensitivity analysis: Restrict BRFSS analysis to only those respondents with ≥2 domains of limitation (approximate proxy for "multiple ADL limitations"). Does the Black–White disability gap persist?

**5. Missing engagement with the economic justification for work requirements**

The paper frames frailty underdetermination as clearly problematic, but does not address the countervailing policy argument: Work requirements' supporters contend that work has health benefits for some non-frail Medicaid beneficiaries, and that precise frailty determination (neither over-inclusive nor under-inclusive) is optimal. The paper does not acknowledge or rebut this argument.

**Suggested edit:** Discussion section could add: "Some policymakers contend that stringent frailty exemption criteria incentivize work participation, which has independent health benefits. However, this must be weighed against documented harms from coverage loss in the underexempted population."

### OVERALL RECOMMENDATION

**MAJOR REVISION required**

The manuscript makes timely policy contributions by documenting racial gaps in work requirement exemption rates and connecting them to claims-based determination architecture. However, for credibility with a Medicaid policy audience:

1. Disaggregate observed vs. modeled exemption rates; present separate analyses.
2. Update program status information to February 2026.
3. Correct the characterization of OBBBA frailty definition (statute delegates to CMS, not predefined).
4. Clarify what fraction of states actually use true claims-based frailty indices vs. ad-hoc condition lookups.
5. Remove or substantially qualify the policy regression given severe underpowering.

The policy implications are sound but need stronger evidentiary foundation.

---

## REVIEWER 3: Health Equity and Disparities Researcher (Johns Hopkins)

### MAJOR CONCERNS

**1. Ecological fallacy: State-level associations cannot support individual-level causal claims about exemption determination**

The manuscript is presented as an "ecological" study (Methods section: "This analysis operates at the state level...findings describe state-population associations; individual-level inferences are not warranted"). Yet the framing throughout conflates state-level associations with individual-level processes:

- The title promises to explain "Racial and Geographic Disparities in Medically Frail Exemption Rates" (individual outcome).
- The Obermeyer audit compares state-level disability prevalence to state-level exemption rates, claiming this reveals whether "Black enrollees require higher disability burden to achieve the same exemption probability" (individual-level claim).
- The conclusion states the disparities "reflect a common mechanism: healthcare utilization data is a poor proxy for health need in populations with structural barriers to care" (individual-level mechanism).

But this is circular: If an individual Black enrollee's exemption probability depends on unmeasured state-level factors (e.g., the distribution of ICD-10 codes in the state population, the composition of primary care physicians), we cannot infer from state-level data that the *individual* is being underdetermined because of suppressed utilization. The individual-level exemption probability is a function of both individual characteristics AND state-level system design.

Example: State A has a racial gap of 5 pp. Is this because Black individuals are underdetermined due to suppressed claims? Or because Black individuals are distributed differently across condition categories than white individuals? State-level data cannot distinguish these. Answering this question requires T-MSIS TAF individual-level data (restricted access).

**Specific equity recommendation:** Reframe the entire manuscript as a state-level descriptive study that raises a hypothesis (suppressed utilization might be one mechanism), but does not claim to have demonstrated it. Add: "These findings do not establish individual-level mechanisms. Individual-level analysis would require examination of T-MSIS claims and demographic data (ResDAC-restricted) linked to exemption decisions."

**2. BRFSS "any-disability" is a poor and unvalidated comparator for claims-based frailty determinations**

The ecological audit compares BRFSS any-disability prevalence (self-reported functional limitation in ≥1 of 6 domains: hearing, vision, cognition, mobility, self-care, independent living) to state exemption rates. The claim is that if Black enrollees have higher BRFSS disability at the same exemption rate, states are underdetermining Black frailty.

But this comparison has severe validity problems:

- **Conceptual misalignment:** BRFSS any-disability is a population-surveillance measure of functional limitation. State frailty definitions are clinical eligibility criteria (often requiring "substantial limitation in multiple ADLs"). These are different constructs. A Black enrollee might report hearing difficulty (BRFSS domain) without meeting the state's threshold of "substantial limitation in ≥2 ADLs." The BRFSS-frailty comparison conflates prevalence surveillance with clinical eligibility.

- **Differential reporting:** Self-reported disability in BRFSS varies by race due to cultural factors in disability framing, healthcare experience, and trust in government surveys. Black respondents may report functional limitations more candidly in BRFSS due to historical experiences with healthcare systems. White respondents may underreport due to different cultural norms. The Black–White disability gap in BRFSS may partly reflect reporting differences rather than true functional differences.

- **No validation linking BRFSS to exemption decisions:** Has any study shown that BRFSS-measured disability predicts who *should* be exempted from work requirements according to clinical criteria? If not, using BRFSS as the gold standard for calibration testing is unvalidated.

**Specific equity recommendation:** Replace the ecological audit with a more modest statement: "BRFSS any-disability prevalence is higher among Black adults than white adults in all 16 study states (mean gap: 6.6 pp), suggesting potentially higher functional limitation burden. However, because BRFSS measures self-reported functional limitation and state exemption criteria use clinical ADL thresholds, direct comparison has limited validity." Remove the claim that the 6.59 pp calibration gap represents misidentification.

**3. Confounding by income, health literacy, and care fragmentation is severe and uncontrolled**

The paper correlates state racial exemption gaps with state-level characteristics (provider density, rurality) but does not control for potential confounders:

- **Income composition:** States with higher Black poverty rates may have both (a) lower personal care provider density (because providers concentrate in higher-income areas), AND (b) lower exemption rates for ALL groups, regardless of claims data architecture. The correlation between provider density and racial gap might be confounded by state-level income inequality.

- **Health literacy and documentation capacity:** States with lower health literacy among Black enrollees may have lower exemption rates not because of claims data bias, but because Black enrollees are less able to navigate physician certification requirements or HIE eligibility determination processes. This is a *bureaucratic burden* problem, not a claims-data-bias problem. The paper does not distinguish these.

- **Care fragmentation and primary care access:** States with more fragmented primary care systems (common in rural areas and areas with high Black poverty) may produce fewer documented diagnoses *independently* of true functional status. This is a healthcare access problem masquerading as a data architecture problem.

**Specific equity recommendation:** Acknowledge in Limitations: "State-level racial gaps may reflect multiple mechanisms including claims data architecture, healthcare access barriers, documentation capacity, and health literacy. This analysis does not disentangle these; causal attribution to claims-based determination is provisional." Add exploratory analysis: Correlate racial gap with state income inequality and primary care physician density (separate from personal care providers).

**4. Framing of "algorithmic bias" is inappropriate for heterogeneous, non-algorithmic state processes**

The manuscript frequently invokes "algorithmic bias" language:
- Title refers to examining states' frailty systems (Methods: "states are implementing claims-based administrative systems...many relying on billing data").
- The ecological audit is called "algorithmic audit" following Obermeyer et al.
- The Conclusions recommend "Algorithmic Impact Assessments."

But *most states do not use algorithms.* States use decision trees: IF ICD-10 code X THEN exempt; IF T1019 billing present THEN eligible for ex parte review. These are *rule-based eligibility criteria*, not machine-learning algorithms. Calling them "algorithmic" invokes algorithmic bias discourse (from ML fairness literature) in a context where the bias mechanism is often *bureaucratic* (physician access, documentation capacity) not algorithmic (statistical discrimination by a learned model).

Michigan may use a true frailty index (claimed in Table 2 as "CFI"), but this is **1 of 17 states**. Applying algorithmic fairness frameworks to the other 16 states misframes the problem.

**Specific equity recommendation:** Distinguish:
- **True algorithmic bias** (Michigan, if CFI is a validated ML model): applies Obermeyer audit, fairness metrics, etc.
- **Rule-based eligibility bias** (other 16 states): driven by rule scope, implementation fidelity, and bureaucratic burden rather than statistical discrimination.

Retitle to: "Claims-based eligibility determination" rather than "algorithmic bias." Restrict "algorithmic fairness" language to Michigan or remove it.

**5. The Obermeyer audit methodology is misapplied at the ecological level**

Obermeyer et al. (2019) demonstrated individual-level algorithmic bias by showing that, among patients at the *same risk score*, Black patients had more chronic conditions than white patients. This proved the algorithm was miscalibrated at the individual level.

The manuscript attempts an ecological analog: comparing Black and white disability prevalence within octiles of *state exemption rates*. But this is not the same test:

- Obermeyer looked at individuals with *identical algorithm scores* and found different clinical needs by race.
- This manuscript looks at states with *similar exemption rate ranges* and finds different average disability prevalence by race. But the individual-level variation within states is unobserved. It's possible that within Georgia, the *individual-level* algorithm is perfectly calibrated, but aggregate Black exemption rates are lower because of Black/white differences in the *distribution* of conditions triggering exemption.

**Specific equity recommendation:** Reframe the ecological audit as: "We find that in states where overall exemption rates are similar, Black adults have higher self-reported disability prevalence than white adults (mean gap: 6.59 pp). This is consistent with, but does not prove, individual-level algorithmic miscalibration." Acknowledge that individual-level data would be needed to definitively answer the calibration question.

### MINOR CONCERNS

**1. "Disparate treatment" vs. "disparate impact" distinction is muddled**

The manuscript implies the disparities are driven by disparate *impact* (claims-based systems have unequal effects regardless of intent). But some of the policy explanations (e.g., physician certification requirements) describe disparate *treatment* (requiring something of one group more than another). The manuscript does not clearly distinguish these legal and conceptual categories.

**Suggested edit:** Add to Discussion: "Under CMS civil rights standards, disparate impact discrimination occurs when facially neutral policies (e.g., claims-based frailty determination) have unequal effects by race. Our findings suggest potential disparate impact. Disparate treatment (intentional differential rule application by race) is not evaluated here."

**2. No discussion of intersectionality — rural Black vs. urban Black vs. rural white enrollees**

The paper analyzes "Black" and "white" as monolithic groups but does not examine whether rural Black enrollees experience different exemption disparities than urban Black enrollees. Given that provider density and rurality are both analyzed, an intersectional analysis (race × geography) would be informative and more precise for equity.

**Suggested edit:** Add exploratory analyses comparing racial gaps in high-provider vs. low-provider regions within states (if state-level geographic data available), or note as future work.

**3. Missing discussion of residential segregation as a confounder/mechanism**

States with higher residential segregation (measured e.g., by dissimilarity index) may have both lower personal care provider density in Black neighborhoods AND lower documented claims for Black enrollees, independent of algorithmic bias. The paper does not address segregation as a structural driver of both the provider density and the exemption gap.

**Suggested edit:** Discussion: "To the extent that provider density proxies for segregated healthcare systems, the association between provider density and racial gap may reflect segregation effects rather than claims-data architecture effects."

**4. No engagement with historical Medicaid racism**

The paper frames disparities as driven by contemporary claims data architecture, but does not situate this within the historical context of Medicaid as a racialized program (documented by scholars like Alina Salganicoff, David Barr). Current exemption rate disparities may reflect accumulated effects of prior discriminatory state policies.

**Suggested edit:** Acknowledge in Discussion: "These disparities occur within a historical context of state-level Medicaid policies with documented racial inequities. Understanding current exemption gaps requires attending to how prior policy choices have shaped enrollment composition and healthcare access patterns by race."

**5. Coverage loss projections lack individual-level grounding**

Table 5 projects "excess Black coverage losses" of 223,833 persons. But this number is derived from an aggregate racial gap without accounting for individuals' actual exemption status, compliance with work requirements, or other factors affecting coverage retention. This projection is presented with scientific precision ("223,833") but derives from ecological data.

**Suggested edit:** Table 5 caption: "These projections are based on aggregate racial gap estimates and assume all individuals in the gap would retain coverage if exempted. Individual-level coverage loss depends on work requirement compliance, documentation capacity, and other factors not captured here."

### OVERALL RECOMMENDATION

**MAJOR REVISION required (with suggestions for substantial reframing)**

The manuscript documents important state-level disparities in exemption rates. However, for publication in a health equity venue, the causal attribution to "algorithmic bias" and suppressed utilization needs major qualification:

1. Clearly separate state-level associations from individual-level mechanisms. Individual-level inference requires T-MSIS data.
2. Replace BRFSS-frailty comparison with more modest claims about disability prevalence alignment.
3. Explicitly enumerate confounders (income inequality, healthcare access, health literacy) that could explain racial gaps without invoking claims-data bias.
4. Distinguish algorithmic bias (rare, perhaps Michigan only) from rule-based eligibility bias and bureaucratic burden (more common).
5. Reframe the Obermeyer audit as a state-level ecological description, not proof of individual-level miscalibration.

The equity implications are significant, but the causal claims require either stronger evidence or more careful framing.

---

## REVIEWER 4: Econometrics and Causal Inference Expert

### MAJOR CONCERNS

**1. The Callaway–Sant'Anna estimator is correctly described but the validity of parallel trends is inadequately assessed**

The paper estimates the causal effect of work requirement adoption using Callaway–Sant'Anna (2021) with treatment cohorts at g=2018, g=2019, g=2023. The parallel trends assumption—that untreated units' outcomes would have evolved in parallel with treated units absent treatment—is tested using pre-treatment placebo tests (pre-treatment ATT ≈ 0, p=0.91).

However, there are two issues:

- **Limited pre-period for 2023 cohort:** The g=2023 cohort (Georgia, North Carolina) has a 7-year pre-period (2016–2022). But the 2018 cohort (Arkansas, Indiana) has only 2 years of pre-data (2016–2017). With n=2 pre-periods, estimating a trend is unreliable. The parallel trends test cannot rule out a coincidental alignment of trends in 2016–2017 followed by divergence.

- **Rambachan–Roth sensitivity is insufficient:** The paper reports Rambachan–Roth (2023) sensitivity analysis showing robustness to parallel trends violations up to 0.4 pp/year. But this bound is **narrow**—it amounts to claiming the ATT of 1.24 pp would remain significant even if the pre-treatment gap was widening by 0.4 pp annually. With only 2 pre-periods, a linear pre-trend of 0.4 pp/year is plausible but unobserved. The sensitivity analysis does not rule out modest violations.

- **Heterogeneous treatment timing confounding:** The paper combines the g=2018 cohort (AR, IN with different characteristics) and g=2023 cohort (GA, NC with different characteristics) in a single treatment effect estimate. Callaway–Sant'Anna is designed for this, but the ATT assumes that the comparison groups (not-yet-treated + never-treated) are valid counterfactuals for both cohorts. If GA/NC's post-2023 trends differ systematically from the 2018 cohort's 2018 trends (e.g., due to different macroeconomic conditions, different CMS guidance), the aggregated ATT could be biased.

**Specific methodological recommendation:**

a) Present separate causal estimates for g=2018 and g=2023 cohorts (already in Table 4, but not emphasized in Results).

b) For the g=2018 cohort, acknowledge: "With only 2 pre-treatment periods (2016–2017), the parallel trends assumption is difficult to verify. Results for this cohort should be interpreted as suggestive." The permutation p from the pre-treatment ATT mean (p=0.91) is not sufficient evidence when the pre-period is this short.

c) Add Figure S1 interpretation: "Pre-treatment ATT estimates cluster near zero for the 2023 cohort (2016–2022, n=7 periods). For the 2018 cohort (2016–2017, n=2 periods), the pre-treatment check is less informative."

**2. The Arkansas synthetic control effectively becomes a two-state comparison; this is inadequately disclosed**

The paper states: "The Arkansas synthetic control converged to a single donor state (California, weight=1.00), effectively reducing this to a two-state comparison...should not be interpreted as a causal estimate." But then the result is reported in the abstract under "causal inference methods" and in the Results section under "Causal Effect of Work Requirement Adoption."

The problem: Reporting a two-state difference (AR vs. CA) as part of a multi-method causal analysis gives it unwarranted credibility. The reader cannot easily distinguish between:

- The Callaway–Sant'Anna ATT of 1.24 pp (credible, n=144 state-years, multiple donor states)
- The Arkansas SCM effect of 0.47 pp (two-state comparison, permutation p=0.818, not significant)

The synthetic control section should be framed as "exploratory case study" not "causal inference."

Additionally, the paper does not explain *why* California is the optimal donor for Arkansas. California is the largest state in the sample, with substantially different demographics, provider infrastructure, and policy environment. The implicit assumption is that California's pre-2018 racial gap trajectory is a valid counterfactual for Arkansas post-2018. This is questionable. Did the authors consider other donor weights? Did alternative donor pools (e.g., other Southern states) produce different results?

**Specific methodological recommendation:**

a) Present the three synthetic control case studies separately from the primary DiD finding. Use a subsection: "Supplementary Case Study Analysis: Synthetic Control" rather than presenting it as co-equal causal evidence.

b) For Arkansas, present alternative donor pool results if available (e.g., synthetic control using only Southern or low-provider-density donor states). If only California is viable, this should be explicitly noted as a limitation of the data rather than a feature of the method.

c) Reframe Results text: "These case studies are exploratory. The aggregate DiD estimate (1.24 pp) based on Callaway–Sant'Anna with multiple control states provides the primary causal estimate."

**3. The regression with N=16 states and 6 predictors should not be presented at all, even as "exploratory"**

The paper presents Table 3 (OLS regression of racial gap on policy variables) with n=16 and 6 predictors, acknowledging it is "severely underpowered." Adjusted R²=0.225 indicates ~78% of variance unexplained.

But presenting regression results with such dire power characteristics, even with a caveat, has a pernicious effect:
- Readers may cite the point estimate (β=−0.51 for stringency) without reading the disclaimer.
- The published table legitimizes under-powered inference.
- The footnote disclaiming the results does not fully undo the visual authority of a regression table.

For comparison, in econometrics, standard practice is to not present regression tables with degrees of freedom <10× the number of predictors. This regression has df ≈ 10 with 6 predictors.

**Specific methodological recommendation:**

**Remove Table 3 entirely.** Instead, present a qualitative comparison: "Among the four states with observed exemption rate data (Georgia, Arkansas, Indiana, North Carolina), we observed the following associations between policy features and racial gaps: [descriptive bullet points]." This acknowledges the limited sample without pretending to statistical inference.

If the authors insist on retaining the regression, then:

a) Move it to Appendix only.

b) Include a sensitivity analysis: Bootstrap the regression 10,000 times, drawing n=8 states (50% sample) without replacement, and compare coefficient distributions. This would illustrate instability.

c) Report the correlation matrix of predictors to diagnose multicollinearity (footnote in Table 3 notes "counterintuitive signs likely reflect multicollinearity").

**4. The panel data structure mixes observed and modeled outcome values without disclosing this in the DiD analysis**

The Callaway–Sant'Anna DiD uses a panel of state-year racial exemption gaps from 2016–2024. But the outcome variable (racial gap) is derived from:

- **Observed data** (4 states: GA, AR, IN, NC) in specific years when exemption evaluations were published.
- **Modeled data** (12 states) across all years, predicted from the OLS regression framework.

The paper does not clearly disclose which years' outcome values are observed vs. modeled for each state. Are Georgia's 2016–2024 racial gaps all observed, or only 2024? Are Indiana's 2016–2024 values all observed or extrapolated?

If the DiD outcome is based on 12 states' *modeled* values across multiple years, the estimate is essentially a DiD of a regression prediction, not a DiD of observed data. This materially changes the interpretation.

**Specific methodological recommendation:**

a) Create a supplementary table: "Outcome Data Provenance by State-Year," indicating which state-year racial gap values are observed vs. modeled.

b) Present two DiD estimates: (i) **Primary:** Using only state-years with observed outcome data (n much smaller, likely n < 50); (ii) **Supplementary:** Using observed + modeled outcomes.

c) In text: "The aggregate ATT of 1.24 pp is based on [X]% observed outcomes and [Y]% modeled outcomes. Sensitivity analysis restricting to observed outcomes only shows [result]."

**5. There is no sensitivity analysis for unmeasured confounding in the DiD specification**

The Callaway–Sant'Anna estimate assumes conditional parallel trends—that conditional on covariates, treatment and comparison groups have parallel trajectory. The paper does not condition on covariates in the presented specification (Appendix B.3 does not mention covariate adjustment).

Without covariate adjustment, the DiD is vulnerable to unmeasured confounding. For example:

- States that adopt work requirements might do so in response to changes in state budget constraints, which might independently affect Medicaid eligibility broadly (reducing exemption rates for all groups).
- States might adopt work requirements in response to political ideology shifts, which might simultaneously affect other administrative stringency.

The paper does not present a covariate-adjusted DiD or any sensitivity analysis for unmeasured confounding.

**Specific methodological recommendation:**

a) Present a covariate-adjusted Callaway–Sant'Anna specification including state-year controls (e.g., state GDP growth, state Medicaid budget as % of state general fund, gubernatorial party). The methods section should specify these controls ex ante.

b) Conduct a Rotnitzky-Vansteelandt robustness test (or similar unmeasured confounding sensitivity analysis) to assess how strong unmeasured confounding would need to be to overturn the ATT estimate.

c) Acknowledge in Limitations: "The DiD assumes no unmeasured confounders associated with work requirement adoption timing and post-adoption racial exemption gap changes. Sensitivity analysis to this assumption is presented in Appendix [X]."

### MINOR CONCERNS

**1. The synthetic control case study sample is selected by treatment status, not ex ante**

The paper analyzes synthetic controls for Georgia 2023, Montana 2019, and Arkansas 2018—all the treated states with race-stratified exemption data. This is ex post selection: the authors chose to do case studies for the states with the best available outcome data.

But synthetic control inference should be pre-specified. If the authors conduct case studies for all treated states, but the worst-fit or least-interpretable case (e.g., Montana with p=0.364) is less likely to be cited, this introduces selective reporting bias.

**Suggested edit:** Add to Methods: "Case studies were pre-specified to include all states with treatment years and ≥5 pre-treatment periods."

**2. Missing description of how the DiD outcome variable was aggregated for multi-state treatment cohorts**

For the g=2018 cohort, both Arkansas and Indiana adopted work requirements in 2018. But they did so at different times within the year (Arkansas June 2018, Indiana unclear). The DiD treats them as the same treatment year, but the timing variation might matter.

**Suggested edit:** Appendix Methods B.3: "Arkansas adopted work requirements effective June 15, 2018; Indiana's implementation date [specify]. Both are assigned treatment year g=2018. Sensitivity analysis varying treatment timing [present if conducted]."

**3. The pre-treatment ATT placebo test p-value (p=0.91) conflates multiple time periods**

The text reports: "Pre-treatment placebo effects are small and statistically indistinguishable from zero (mean pre-treatment ATT: −0.023 pp, SE=0.202; p=0.91)." This is a test that the *mean* pre-treatment ATT across all pre-periods equals zero, but it does not test whether *individual* pre-treatment periods show a trend.

For example, if ATT(g=2018, 2016)=+0.50 pp and ATT(g=2018, 2017)=−0.54 pp, the mean is −0.023 pp (p=0.91 overall), but there is underlying volatility that could reflect an unobserved pre-trend.

**Suggested edit:** Appendix B.3 or Figure S1 should show the full time series of ATT(g,t) for all pre-treatment periods, not just report the mean. Visual inspection helps diagnose pre-trends.

**4. No justification for octile binning in the ecological audit (Appendix B.1)**

The ecological audit divides 16 states into 8 octiles (2 states per octile). But why octiles? The choice of bin size affects results. With n=16 states, alternative binning (e.g., quartiles with 4 states per bin, or quintiles with 3–4 states) would be more stable.

**Suggested edit:** Appendix B.1: "We chose octile binning to match the original Obermeyer et al. (2019) study design. Sensitivity analysis with quartile binning shows [result]."

**5. The 95% CIs for the ATT are bootstrap CIs; the method should be specified**

The paper reports "95% CI: 0.80–1.68" for the aggregate ATT but does not specify the CI construction method. Appendix B.3 mentions "999 bootstrap replicates," suggesting percentile bootstrap. But was it percentile, bias-corrected and accelerated (BCa), or other?

**Suggested edit:** Methods, causal inference subsection: "95% CIs for ATT(g,t) are bias-corrected and accelerated (BCa) bootstrap intervals with 999 replicates, accounting for heteroskedasticity and potential non-normal bootstrap distributions."

### OVERALL RECOMMENDATION

**MAJOR REVISION required**

The paper applies appropriate modern causal inference methods (Callaway–Sant'Anna) to the state-level policy question, but has several methodological issues that compromise credibility:

1. **Parallel trends verification is weak for the 2018 cohort** (2-year pre-period) and rests on narrow Rambachan–Roth bounds.
2. **The Arkansas synthetic control is a two-state comparison** and should not be presented as co-equal causal evidence. Reframe as supplementary exploratory case study.
3. **The policy regression (Table 3) is severely underpowered and should be removed or relocated to Appendix** with strong qualifications.
4. **The DiD outcome variable structure** (mixing observed and modeled values) is inadequately disclosed. Present observed-only and observed+modeled estimates separately.
5. **No covariate adjustment or unmeasured confounding sensitivity analysis** in the DiD specification.

The aggregate ATT of 1.24 pp (95% CI: 0.80–1.68) from Callaway–Sant'Anna is credible for the aggregate effect, but the above issues should be addressed for publication in a top econometrics venue.

---

## SUMMARY TABLE: Cross-Reviewer Consensus

| Issue | Reviewer 1 (Policy) | Reviewer 2 (Medicaid Expert) | Reviewer 3 (Equity) | Reviewer 4 (Econometrics) |
|-------|---|---|---|---|
| **Data quality: Modeled vs. observed outcomes** | Implicit concern | **MAJOR** | Minor | **MAJOR** |
| **Causal mechanism claims overstate evidence** | **MAJOR** | Minor | **MAJOR** | N/A |
| **Regression with n=16 is underpowered** | Implicit | **MAJOR** | Minor | **MAJOR** |
| **Arkansas synthetic control is weak** | N/A | N/A | N/A | **MAJOR** |
| **Parallel trends verification insufficient** | N/A | N/A | N/A | **MAJOR** |
| **Ecological fallacy / individual-level inference** | Minor | N/A | **MAJOR** | N/A |
| **BRFSS validity as comparator** | N/A | **MAJOR** | **MAJOR** | N/A |
| **Confounding uncontrolled** | N/A | Minor | **MAJOR** | Minor |
| **OBBBA characterization inaccurate** | **MAJOR** | **MAJOR** | N/A | N/A |
| **"Algorithmic bias" framing inappropriate** | Minor | **MAJOR** | **MAJOR** | N/A |

---

## CROSS-CUTTING RECOMMENDATIONS FOR REVISION

**If the authors revise for resubmission, they should:**

1. **Disaggregate observed vs. modeled exemption rates throughout.** Present primary results using observed data only (n=4 states), with observed+modeled as sensitivity analysis.

2. **Correct the OBBBA frailty definition statement.** The statute delegates to CMS; it does not predefined criteria.

3. **Reframe causal claims as hypotheses.** Replace "reflects a mechanism of suppressed utilization" with "is consistent with a hypothesis that claims-based determination underidentifies individuals with suppressed healthcare utilization; alternative explanations cannot be excluded."

4. **Remove or radically qualify Table 3 (policy regression).** Either move to Appendix with strong disclaimer or replace with qualitative description of observed-data states.

5. **Reframe the Arkansas synthetic control** as an exploratory case study, not a primary causal estimate.

6. **Add explicit sensitivity analyses:**
   - Callaway–Sant'Anna restricted to g=2023 cohort (longer pre-period)
   - DiD with covariate adjustment
   - Ecological audit restricted to observed-data states (n=4)

7. **Acknowledge the ecological-to-individual inference gap.** State explicitly: "These findings describe state-level associations. Individual-level mechanisms would require T-MSIS claims-linked exemption data."

8. **Qualify BRFSS comparability.** Acknowledge that BRFSS any-disability does not perfectly align with state clinical frailty definitions and the calibration test is approximate.

9. **Update program status to February 2026** and discuss implications of any post-2024 changes.

10. **Strengthen the policy feasibility section.** Address CMS authority, state pushback, and enforcement mechanisms for the recommended policy changes.

---

**CONSENSUS RECOMMENDATION ACROSS ALL FOUR REVIEWERS:**

### MAJOR REVISION REQUIRED (with suggestion to resubmit after revisions)

**Verdict:** The manuscript tackles an important and timely policy question (racial disparities in Medicaid work requirement exemptions) using multiple valid methods (ecological calibration audit, staggered DiD, synthetic control). The core findings—that Black enrollees are exempted at lower rates despite equal or higher disability burden, and that this gap increases with work requirement adoption—are substantively important for health policy and equity.

However, the manuscript has methodological, data quality, and framing issues that must be addressed before publication:

**Critical issues (must fix):**
- Disaggregate observed vs. modeled exemption rates; present separate estimates
- Remove or radically qualify the underpowered policy regression
- Reframe causal mechanism claims as hypotheses
- Correct OBBBA statutory characterization

**Important issues (should fix):**
- Strengthen parallel trends verification for 2018 cohort
- Reframe Arkansas SCM as supplementary case study
- Acknowledge ecological-to-individual inference gap
- Add covariate-adjusted DiD and unmeasured confounding sensitivity

**After addressing these, the manuscript would be suitable for publication** as a significant contribution to health policy and health equity research, with appropriately qualified causal claims and transparent data limitations.